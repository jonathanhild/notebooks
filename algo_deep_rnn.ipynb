{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "welsh-lemon",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "\n",
    "## Advantages\n",
    "\n",
    "- RNNs work well with **sequential data** where order matters.\n",
    "- Useful in Natural Language Processing applications.\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "- Same as general neural network disadvantages.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Fundamentals of Deep Learning, Chapter 7.\n",
    "1. Hands-On Machine Learning, Chapter 15, 16.\n",
    "1. Practical Natural Language Processing.\n",
    "1. Multi-class Text Classification with LSTM: [https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17](https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6290df0-c81b-4f32-a7a1-09fe82164ff5",
   "metadata": {},
   "source": [
    "# Gated Recurrent Unit (GRU)\n",
    "\n",
    "**Gated recurrent units** can combine new information $x_t$ with internal state $h_{t-1}$ from the previous step.\n",
    "\n",
    "Gates use **sigmoid** activation $\\sigma$ and **element-wise product** $\\odot$.\n",
    "\n",
    "Update gate:\n",
    "\n",
    "$$\n",
    "    z_t = \\sigma(W_z \\cdot [h_{t-1} x_t])\n",
    "$$\n",
    "\n",
    "Reset gate:\n",
    "\n",
    "$$\n",
    "    r_t = \\sigma(W_r \\cdot [h_{t-1} x_t])\n",
    "$$\n",
    "\n",
    "Current memory:\n",
    "\n",
    "$$\n",
    "    \\tilde{h_t} = \\tanh(W \\cdot [r_t \\cdot h_{t-1} x_t])\n",
    "$$\n",
    "\n",
    "Internal state (output):\n",
    "\n",
    "$$\n",
    "    h_t = (1 - z_t) \\cdot h_{t-1} + z_t \\cdot \\tilde{h_t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de3334-75de-4274-a4c8-69113c487cb1",
   "metadata": {},
   "source": [
    "# LSTM: Long-Short Term Memory\n",
    "\n",
    "## The Short-term Memory Problem\n",
    "\n",
    "## Process\n",
    "\n",
    "What information we want to forget:\n",
    "\n",
    "$$\n",
    "    f_t = \\sigma (W_f \\cdot [h_{t-1},x_t] + b_f)\n",
    "$$\n",
    "\n",
    "What new information we want:\n",
    "\n",
    "$$\n",
    "    i_t = \\sigma (W_i \\cdot [h_{t-1}, x_t] + b_i)\\\\\n",
    "    \\tilde{C_t} = \\tanh (W_C \\cdot[h_{t-1}, x_t] + b_C)\n",
    "$$\n",
    "\n",
    "Update the old cell state:\n",
    "\n",
    "$$\n",
    "    C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C_t}\n",
    "$$\n",
    "\n",
    "What do we output:\n",
    "\n",
    "$$\n",
    "    O_t = \\sigma (W_O [h_{t-1}, x_t] + b_O\\\\\n",
    "    h_t = O_t \\cdot \\tanh(C_t)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604b2a5-6b7f-4623-bf59-62d2c9777646",
   "metadata": {},
   "source": [
    "# LSTM with Peephole Variance\n",
    "\n",
    "Adds peepholes to all of the gates. Allows each gate to see the previous state.\n",
    "\n",
    "$$\n",
    "    f_t = \\sigma(W_f \\cdot [C_{t-1}, h_{t-1} x_t] + b_f)\\\\\n",
    "    i_t = \\sigma(W_i \\cdot [C_{t-1}, h_{t-1} x_t] + b_i)\\\\\n",
    "    O_t = \\sigma(W_O \\cdot [C_{t-1}, h_{t-1} x_t] + b_O)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd4080-2c6f-40ac-b067-ca0a47c6b678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datascience]",
   "language": "python",
   "name": "conda-env-datascience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
