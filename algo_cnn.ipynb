{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c2721b-b6ce-4d27-a91b-f0eec70a06cd",
   "metadata": {},
   "source": [
    "# Convolution Neural Networks (CNNs)\n",
    "\n",
    "## Pros\n",
    "\n",
    "- Used in image classification: facial recognition, satellite analysis, optical character recognition, sound analysis through audiograms, image tagging, anomaly detection in video images.\n",
    "- Can use large, full-color images.\n",
    "- CNNs preserve **locality** of pixels in an image (all the pixels in a small vicinity have relationships with each other).\n",
    "- CNNs reduces the connections so we can go deeper: done through shared weights used by **filters** (aka **kernels**).\n",
    "- **Transfer learning** can be used to extend a model to new image classification with small amount of retraining.\n",
    "\n",
    "## Cons\n",
    "\n",
    "- Sames as general neural network disadvantages.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Hands-On Machine Learning, Chapter 14, 15.\n",
    "1. Fundamentals of Deep Learning, Chapter 5.\n",
    "1. Deep Learning, Chapter 9.\n",
    "1. A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way: [https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)\n",
    "1. Wikipedia: Convolutional Neural Network: [https://en.wikipedia.org/wiki/Convolutional_neural_network](https://en.wikipedia.org/wiki/Convolutional_neural_network)\n",
    "1. StatQuest: Convolutional Neural Networks: [https://www.youtube.com/watch?v=HGwBXDKFk9I](https://www.youtube.com/watch?v=HGwBXDKFk9I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a253bfc-5f02-4aab-abd4-f9c33d8dfadb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Convolutional Layers\n",
    "\n",
    "## Filters\n",
    "\n",
    "A **convolutional layer** takes a subsample of a matrix by applying a **filter** or **kernel**. The filter is commonly a 3x3, 5x5, or 7x7 grid. The filter is overlayed onto the full matrix the dot product is made. A bias is then added.\n",
    "\n",
    "Output of a neuron in a convolution layer,\n",
    "\n",
    "$$\n",
    "    z_{i,j,k} = b_k + \\sum_{u=0}^{f_h-1} \\sum_{v=0}^{f_w-1} \\sum_{k^\\prime=0}^{f_{n^\\prime -1}} x_{i^\\prime,j^\\prime,k^\\prime} \\cdot w_{u,v,k^\\prime,k} \\text{ with } \\begin{cases} i^\\prime = i \\times s_h + u \\\\ j^\\prime = j \\times s_w + v\\end{cases}\n",
    "$$\n",
    "\n",
    "- $z_{i,j,k}$ is the output of the neuron located in row $i$, column $j$ in feature map $k$ of the convolutional layer $l$.\n",
    "- $s_h$ and $s_w$ are the vertical and horizontal strides, $f_h$ and $f_w$ are the height and width of the receptive field, and $f_{n^\\prime}$ is the number of feature mapts in the previous layer (layer $l - 1$).\n",
    "- $x_{i^\\prime,j^\\prime,k^\\prime}$ is the output of the neuron located in layer $l – 1$, row $i^\\prime$, column $j^\\prime$, feature map $k^\\prime$ (or channel $k^\\prime$ if the previous layer is the input layer).\n",
    "- $b_k$ is the bias term for feature map $k$ (in layer $l$). You can think of it as a knob that tweaks the overall brightness of the feature map $k$.\n",
    "- $w_u,v,k^\\prime,k$ is the connection weight between any neuron in feature map $k$ of the layer $l$ and its input located at row $u$, column $v$ (relative to the neuron’s receptive field), and feature map $k^\\prime$.\n",
    "\n",
    "### Stride\n",
    "\n",
    "**Stride** \n",
    "\n",
    "### Padding\n",
    "\n",
    "**Padding** \n",
    "\n",
    "## Feature Map\n",
    "\n",
    "A **feature map** is the composition of all filter dot products over the entire matrix. The feature map is then run through a ReLU activation function.\n",
    "\n",
    "## Pooling Layers\n",
    "\n",
    "**Max pooling** uses a new filter on the feature map and selects the region(s) with the highest possible values. In other words, max pooling identifies which mapped features most closely matched the input.\n",
    "\n",
    "### Average Pooling\n",
    "\n",
    "Alternatively, **mean pooling** calculates the average for each region.\n",
    "\n",
    "### Depthwise Pooling\n",
    "\n",
    "### Global Average Pooling\n",
    "\n",
    "## Neural Network\n",
    "\n",
    "The pooled feature map become the inputs for a classification neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e5e1b-924f-40be-8928-d42f0f233b42",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network with Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa31a799-09bf-4e85-9de7-3a7a0ed60945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import erf\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24430e76-fe0d-4ab3-8589-58a9cb6e5f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb68a31-e0b6-4f16-a5f3-2547bc06efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_valid = X_valid / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_valid = X_valid.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c36e255d-f869-4433-8ee4-56c164838342",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = keras.models.Sequential(\n",
    "    [\n",
    "        keras.layers.Conv2D(64, 7, activation='relu', padding='same', input_shape=(28,28,1)),\n",
    "        keras.layers.MaxPooling2D(2),\n",
    "        keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "        keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "        keras.layers.MaxPooling2D(2),\n",
    "        keras.layers.Conv2D(256, 3, activation='relu', padding='same'),\n",
    "        keras.layers.Conv2D(256, 3, activation='relu', padding='same'),\n",
    "        keras.layers.MaxPooling2D(2),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a40ece0-4872-45ce-a772-da912ef9f90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(), \n",
    "    loss=keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=[keras.metrics.sparse_categorical_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "479ae573-074a-4ec1-ad6c-7b7df488dbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 64)        3200      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               295040    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 1,413,834\n",
      "Trainable params: 1,413,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bed8f557-6604-40e1-b2bf-71e00b7cbd8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 112s 65ms/step - loss: 0.7359 - sparse_categorical_accuracy: 0.7358 - val_loss: 0.3592 - val_sparse_categorical_accuracy: 0.8704\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 111s 65ms/step - loss: 0.4147 - sparse_categorical_accuracy: 0.8598 - val_loss: 0.3297 - val_sparse_categorical_accuracy: 0.8728\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 111s 65ms/step - loss: 0.3521 - sparse_categorical_accuracy: 0.8799 - val_loss: 0.3025 - val_sparse_categorical_accuracy: 0.8960\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 113s 66ms/step - loss: 0.3177 - sparse_categorical_accuracy: 0.8917 - val_loss: 0.2929 - val_sparse_categorical_accuracy: 0.8950\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 113s 66ms/step - loss: 0.2972 - sparse_categorical_accuracy: 0.9003 - val_loss: 0.2665 - val_sparse_categorical_accuracy: 0.9038\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 114s 66ms/step - loss: 0.2737 - sparse_categorical_accuracy: 0.9070 - val_loss: 0.2689 - val_sparse_categorical_accuracy: 0.9086\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 113s 66ms/step - loss: 0.2602 - sparse_categorical_accuracy: 0.9108 - val_loss: 0.2534 - val_sparse_categorical_accuracy: 0.9128\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 112s 65ms/step - loss: 0.2412 - sparse_categorical_accuracy: 0.9176 - val_loss: 0.2481 - val_sparse_categorical_accuracy: 0.9122\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 112s 65ms/step - loss: 0.2352 - sparse_categorical_accuracy: 0.9200 - val_loss: 0.2633 - val_sparse_categorical_accuracy: 0.9140\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 112s 65ms/step - loss: 0.2209 - sparse_categorical_accuracy: 0.9255 - val_loss: 0.2546 - val_sparse_categorical_accuracy: 0.9172\n"
     ]
    }
   ],
   "source": [
    "history = cnn.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=10, \n",
    "    validation_data=(X_valid, y_valid)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datascience]",
   "language": "python",
   "name": "conda-env-datascience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
