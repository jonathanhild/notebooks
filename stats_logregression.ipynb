{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Logistic Regression\n",
    "Logistic regression is used to model relationships between independent variables and categorical dependent variables. Instead of regression it would more appropriately be called logistic classification. Output of a logistic model is probability that a data point belongs to a class.\n",
    "\n",
    "Classification: Predicting the probability that a data point belongs to a specific class (label).  \n",
    "Logit: shortened name for logistic model.\n",
    "\n",
    "### Assumptions of Logistic Regression\n",
    "- Logit does not make many of the key assumptions of linear models that are based on ordinary least squares algorithms.\n",
    "    - Does not require a linear relationship between the dependent and independent variables.\n",
    "    - Error terms (residuals) do not need to be normally distributed Homoscedasticity is not required.\n",
    "    - Finally, the dependent variable in logistic regression is not measured on an interval or ratio scale (aka not continuous)\n",
    "- What assumptions do apply?\n",
    "    - Dependent variable must follow a binomial distribution\n",
    "    - Observations to be independent of each other.  In other words, the observations should not come from repeated measurements or matched data\n",
    "    - Little or no multicollinearity among the independent variables.  This means that the independent variables should not be too highly correlated with each other.\n",
    "    - Assumes linearity of independent variables and log odds\n",
    "\n",
    "### The Sigmoid Function\n",
    "A sigmoid curve is used to fit the data with the decision boundary separating the classes.\n",
    "$$\n",
    "    \\log(\\eta) = \\frac{1}{1+\\exp(-\\eta)}\n",
    "$$\n",
    "\n",
    "Where,  \n",
    "The probability is bound, $0 \\leq \\text{sigmoid}(x) \\leq 1$  \n",
    "Unlike linear reg. where values can range from $\\pm\\infty$\n",
    "- where x goes to $-\\infty$, $y=0$\n",
    "- where x goes to $+\\infty$, $y=1$\n",
    "\n",
    "![](./img/logit_sigmoid.png)\n",
    "\n",
    "### Log Odds\n",
    "$$\n",
    "    p(x) = \\frac{1}{1+e^{-(\\beta X)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\beta X = \\log\\left(\\frac{p(x)}{1-p(x)}\\right)\n",
    "$$\n",
    "\n",
    "Where,  \n",
    "$p(x)$: probability that an observation belongs to a class.  \n",
    "$-\\beta X$: input to the function (algorithm's prediction, e.g. $mx+b$).  \n",
    "$e$: base of natural log.\n",
    "\n",
    "### Steps in Logistic Regression\n",
    "1. Set a threshold value to distinguish between two classes, for example: \n",
    "    - < 0.5 = 0\n",
    "    - \\> 0.5 = 1\n",
    "    - But it's not that simple\n",
    "1. Use a regression line to create line of best fit for distinguishing between two classes:\n",
    "    - Examine distance between line and each data point\n",
    "1. Plug regression formula into logit.\n",
    "1. Get output prediction for each data point.\n",
    "\n",
    "### Calculating Probability with The Sigmoid Function\n",
    "What are the odds that Y belongs to a particular class?\n",
    "$$\n",
    "    P(Y=1|X)\n",
    "$$\n",
    "\n",
    "For the points near the line, what are the odds that the distance between the point and the line (the error) is greater than 0?\n",
    "$$\n",
    "    P(\\beta X + \\epsilon > 0|X) = P(\\epsilon > -(\\beta X)|X)\n",
    "$$\n",
    "\n",
    "The error terms follow a logistic distribution, so we can use the CDF of the logistic distribution to integrate the probability, which is the sigmoid function.\n",
    "$$\n",
    "    P(Y=1|X) = \\frac{1}{1+e^{-(\\beta X)}}\n",
    "$$\n",
    "\n",
    "![](./img/logit_logodds.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function: Cross Entropy\n",
    "Cross entropy measure the difference between two probability distributions for a given set of events:\n",
    "- $P(y)=0$\n",
    "- $P(y)=1$\n",
    "\n",
    "$$\n",
    "    J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}Cost(h_{\\theta}(x^{(i)}), y^{(i)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "    J(\\theta) =\n",
    "    \\begin{cases}\n",
    "    Cost(h_{\\theta}(x),y) = -\\log(h_{\\theta}(x)) &\\text{if }y=1 \\\\\n",
    "    Cost(h_{\\theta}(x),y) = -\\log(1-h_{\\theta}(x)) &\\text{if }y=0\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "This graph shows the outcome of log loss a wrong and more confident answer (higher probability) gets a greater penalty.\n",
    "\n",
    "![](./img/cross_entropy.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datascience]",
   "language": "python",
   "name": "conda-env-datascience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
